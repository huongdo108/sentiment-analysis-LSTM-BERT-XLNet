{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer Learning with XLNET - Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T13:45:17.264971Z",
     "start_time": "2020-07-17T13:45:15.501321Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GvXz1qP5NpmT"
   },
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer,XLNetForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from transformers import XLNetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMO7ZEp3PlB6"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T13:45:34.999710Z",
     "start_time": "2020-07-17T13:45:33.735730Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "L39VCgxlNpme"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('.\\processed_data\\processed_data.csv',index_col='Unnamed: 0')\n",
    "labels = pd.read_csv('.\\processed_data\\processed_labels.csv',index_col='Unnamed: 0')\n",
    "\n",
    "data = data.rename(columns={\"0\": 'reviews'})\n",
    "labels = labels.rename(columns={\"0\": 'sentiment'})\n",
    "\n",
    "labels =np.array([1 if x =='positive' else 0 for x in labels['sentiment'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define custom dataset and data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T13:51:46.939922Z",
     "start_time": "2020-07-17T13:51:46.292633Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-gBTMQVUNpmh"
   },
   "outputs": [],
   "source": [
    "pre_trained_model = 'xlnet-base-cased'\n",
    "tokenizer = XLNetTokenizer.from_pretrained(pre_trained_model,do_lower_case=True)\n",
    "class XLNetTorchDataset(Dataset):\n",
    "    def __init__(self,data,labels,tokenizer,max_len):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        review = str(self.data[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(review,\n",
    "                                              add_special_tokens=True,\n",
    "                                              truncation=True,\n",
    "                                              max_length = self.max_len,\n",
    "                                              return_token_type_ids=False,\n",
    "                                              pad_to_max_length=True,\n",
    "                                              return_attention_mask=True,\n",
    "                                              return_tensors='pt'\n",
    "                                              \n",
    "                                             )\n",
    "        \n",
    "        return{\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T13:51:47.065111Z",
     "start_time": "2020-07-17T13:51:46.942921Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KwTEc4liNpmj"
   },
   "outputs": [],
   "source": [
    "def prepare_data_loader(data,labels, tokenizer, max_len=250, batch_size=30):\n",
    "    dataset = XLNetTorchDataset(\n",
    "        data=np.squeeze(np.array(data)),\n",
    "        labels=np.array(labels),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(dataset,batch_size=batch_size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,labels,test_size=0.2)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test,y_test,test_size=0.4)\n",
    "train_loader = prepare_data_loader(X_train,y_train, tokenizer, max_len=250, batch_size=30)\n",
    "test_loader = prepare_data_loader(X_test, y_test, tokenizer,max_len=250, batch_size=30)\n",
    "val_loader = prepare_data_loader(X_val, y_val, tokenizer,max_len=250, batch_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define XLNET model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T13:54:33.932998Z",
     "start_time": "2020-07-17T13:54:33.928022Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5H3cH1bbNpmn"
   },
   "outputs": [],
   "source": [
    "class XLNetSentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(XLNetSentimentClassifier, self).__init__()\n",
    "        self.XLNet =  XLNetForSequenceClassification.from_pretrained(pre_trained_model)\n",
    "        # self.drop = nn.Dropout(p=0.1)\n",
    "        #self.out = nn.Linear(self.XLNet.config.hidden_size, n_classes)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs= self.XLNet(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        # output = self.drop(outputs[0])\n",
    "        return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T13:54:38.808865Z",
     "start_time": "2020-07-17T13:54:34.315625Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "2k1XDuZ_Npmp",
    "outputId": "394c6751-5332-4864-bb8c-f0514fa2558d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = XLNetSentimentClassifier(n_classes=2)\n",
    "model = model.to(device)\n",
    "n_epochs = 2\n",
    "total_steps = len(train_loader) * n_epochs\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
    "criterion= nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define train, evaluation, and test functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T17:26:02.517287Z",
     "start_time": "2020-07-15T17:26:02.513254Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "AvTsyz92Npmu"
   },
   "outputs": [],
   "source": [
    "def train_model(model,optimizer,criterion,scheduler,train_loader):\n",
    "    model = model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        input_ids = data['input_ids']\n",
    "        attention_mask = data['attention_mask']\n",
    "        targets = data['label']\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=input_ids,attention_mask=attention_mask).to(device)\n",
    "        #print(outputs.shape)\n",
    "        loss = criterion(outputs,targets).to(device)\n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "        acc = torch.sum(pred == targets)\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss/len(train_loader), epoch_acc/len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nsrt0XSUBrmw"
   },
   "outputs": [],
   "source": [
    "def evaluation_model(model,optimizer,criterion,val_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            input_ids = data['input_ids']\n",
    "            attention_mask = data['attention_mask']\n",
    "            targets = data['label']\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids,attention_mask=attention_mask).to(device)\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            acc = torch.sum(pred == targets)\n",
    "\n",
    "            loss = criterion(outputs,targets).to(device)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return  epoch_loss/len(val_loader), epoch_acc/len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DtIJMKqqCvi"
   },
   "outputs": [],
   "source": [
    "def test_model(model,optimizer,criterion,test_loader):\n",
    "    the_model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            input_ids = data['input_ids']\n",
    "            attention_mask = data['attention_mask']\n",
    "            targets = data['label']\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids,attention_mask=attention_mask).to(device)\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            acc = torch.sum(pred == targets)\n",
    "\n",
    "            loss = criterion(outputs,targets).to(device)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return  epoch_loss/len(test_loader), epoch_acc/len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T17:29:00.011398Z",
     "start_time": "2020-07-15T17:26:03.291760Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "YlME45gRNpmz",
    "outputId": "002baf24-2b4d-4f93-fc6b-d13a958c4930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.227 Train Acc: 91.30% Val Loss: 0.175 Val Acc: 93.73%\n",
      "Epoch: 2 Train Loss: 0.119 Train Acc: 96.12% Val Loss: 0.211 Val Acc: 93.75%\n",
      "Epoch: 3 Train Loss: 0.084 Train Acc: 97.40% Val Loss: 0.211 Val Acc: 93.75%\n",
      "Epoch: 4 Train Loss: 0.084 Train Acc: 97.47% Val Loss: 0.211 Val Acc: 93.75%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 4\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss, train_acc = train_model(model,optimizer,criterion,scheduler,train_loader)\n",
    "    val_loss, val_acc = evaluation_model(model,optimizer,criterion,val_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} Train Loss: {train_loss:.3f} Train Acc: {train_acc*100:.2f}% Val Loss: {val_loss:.3f} Val Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save trained parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIKvB6tmp3-l"
   },
   "outputs": [],
   "source": [
    "PATH = \"trained_XLNet.pt\"\n",
    "torch.save(model.state_dict(),PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload trained parameters and to test the model performance on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "X_9L7DU4p8uV",
    "outputId": "d88ae22d-573e-4ee3-c607-631d62572bd9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model = XLNetSentimentClassifier(n_classes=2)\n",
    "the_model.load_state_dict(torch.load(\"trained_XLNet.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iUMLRVklqEtB",
    "outputId": "9861caae-7fec-43e8-f537-89df70a4e392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.223 Test Acc: 93.32%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = test_model(model,optimizer,criterion,test_loader)\n",
    "print(f'Test Loss: {test_loss:.3f} Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "XLNET.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
