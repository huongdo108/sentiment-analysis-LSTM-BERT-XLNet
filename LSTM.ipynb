{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bidirectional LSTM - Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T07:03:56.764932Z",
     "start_time": "2020-07-18T07:03:54.378822Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xv-ekFgj8pbz",
    "outputId": "b353d183-32f0-4c5f-9ce8-2d78e84d4d0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOh6GcKH-e-4"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T07:24:09.907880Z",
     "start_time": "2020-07-18T07:24:08.748982Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yyp_ClOS8pb7"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('.\\processed_data\\processed_data.csv',index_col='Unnamed: 0')\n",
    "labels = pd.read_csv('.\\processed_data\\processed_labels.csv',index_col='Unnamed: 0')\n",
    "\n",
    "data = data.rename(columns={\"0\": 'reviews'})\n",
    "labels = labels.rename(columns={\"0\": 'sentiment'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess data, define collate function and data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T07:31:16.507196Z",
     "start_time": "2020-07-18T07:31:16.498193Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Q02ul5k18pb9"
   },
   "outputs": [],
   "source": [
    "def preprocessed_data(data,labels):\n",
    "    \"\"\"\n",
    "    Prepare data for the mode\n",
    "    Standard preprocessing: lower case, remove punctuation\n",
    "    Encoding: transform text to numeric representation\n",
    "    Remove outliers\n",
    "    Pad reviews to have the same length\n",
    "    \"\"\"\n",
    "    # lower case and get rid of punctuation\n",
    "    data['reviews'] = data['reviews'].apply(lambda x: x.lower())\n",
    "    data['reviews'] = data['reviews'].apply(lambda x: ''.join([i for i in x if i not in punctuation]))\n",
    "    \n",
    "    # create a list of words\n",
    "    list_words = [x.split() for x in data['reviews']]\n",
    "    words = [word for l in list_words for word in l]\n",
    "    \n",
    "    # build word dictionary that maps words to integers\n",
    "    # the most frequent words will have the smallest index\n",
    "    counts = Counter(words)\n",
    "    vocab = sorted(counts,key=counts.get,reverse=True)\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "    \n",
    "    # use the built dictionary to encode each review in the data to numeric representation\n",
    "    encoded_reviews = [[vocab_to_int[word] for word in l] for l in list_words]\n",
    "    \n",
    "    # encode labels to numeric representation\n",
    "    encoded_labels = np.array([1 if x =='positive' else 0 for x in labels['sentiment'].values])\n",
    "    \n",
    "    # remove outlier (reviews that have zero length)\n",
    "    review_indx_non_zero = [idx for idx,review in enumerate(encoded_reviews) if len(review) != 0]\n",
    "    encoded_reviews = [encoded_reviews[idx] for idx in review_indx_non_zero]\n",
    "    encoded_labels = [encoded_labels[idx] for idx in review_indx_non_zero]\n",
    "    \n",
    "    list_of_samples = [(torch.LongTensor(encoded_reviews[i]),encoded_labels[i]) for i in range(len(encoded_labels))]\n",
    "\n",
    "    \n",
    "    return len(vocab_to_int)+1,list_of_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T08:40:08.933603Z",
     "start_time": "2020-07-18T08:40:08.927605Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fnGoCcpT8pcD"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "padding_value = 0\n",
    "def collate(list_of_samples):\n",
    "    \"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "    Args:\n",
    "      list_of_samples is a list of tuples (src_seq, tgt_seq):\n",
    "          src_seq is of shape (src_seq_length,)\n",
    "          tgt_seq is of shape (tgt_seq_length,)\n",
    "\n",
    "    Returns:\n",
    "      src_seqs of shape (max_src_seq_length, batch_size): Tensor of padded source sequences.\n",
    "          The sequences should be sorted by length in a decreasing order, that is src_seqs[:,0] should be\n",
    "          the longest sequence, and src_seqs[:,-1] should be the shortest.\n",
    "      src_seq_lengths: List of lengths of source sequences.\n",
    "      tgt_seqs of shape (1, batch_size): Tensor of target sequences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    sorted_list = sorted(list_of_samples, key =  lambda x: len(x[0]),reverse=True)\n",
    "    \n",
    "    \n",
    "    #src_seq = pad_sequence([sample[0] for sample in sorted_list],padding_value=padding_value)\n",
    "    src_seq = pad_sequences([sample[0] for sample in sorted_list], \n",
    "                            maxlen=250, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "\n",
    "    src_seq_lengths = [len(sample[0]) if len(sample[0]) < 250 else 250 for sample in sorted_list]\n",
    "    \n",
    "    tgt_seqs = torch.from_numpy(np.array([x[1] for x in sorted_list]))\n",
    "\n",
    "    \n",
    "    return src_seq,src_seq_lengths,tgt_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T08:40:10.446779Z",
     "start_time": "2020-07-18T08:40:10.441814Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MUZpZoFf8pcF"
   },
   "outputs": [],
   "source": [
    "def prepare_data_loader(list_of_samples,train_size,test_size):\n",
    "    \n",
    "    train_idx=round(len(list_of_samples)*train_size)\n",
    "    test_idx= round((len(list_of_samples) - train_idx)*test_size)\n",
    "\n",
    "    \n",
    "    train,remaining = list_of_samples[:train_idx],list_of_samples[train_idx:]\n",
    "    \n",
    "\n",
    "    test,val = remaining[:test_idx],remaining[test_idx:]\n",
    "    \n",
    "    train_loader = DataLoader(train, shuffle=True, batch_size=30,collate_fn=collate, pin_memory=True)\n",
    "    test_loader = DataLoader(test, shuffle=True, batch_size=30,collate_fn=collate, pin_memory=True)\n",
    "    val_loader = DataLoader(val, shuffle=True, batch_size=30,collate_fn=collate, pin_memory=True)   \n",
    "    return train_loader, test_loader, val_loader    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Bidirectional LSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T08:40:10.965675Z",
     "start_time": "2020-07-18T08:40:10.951641Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "suAmgIhN8pcM"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentBidirectLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The Bidirectional Multilayers LSTM model that will be used to perform Sentiment analysis.\n",
    "    \n",
    "    vocab_size: vocabulary size (train? or train +test + val?)\n",
    "    output_size: size of outputs. In this case, the label is either '1' or '0' so output_size=1\n",
    "    embedding_dim: Number of columns in the embedding lookup table; size of our embeddings\n",
    "    hidden_dim: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. \n",
    "                Common values are 128, 256, 512, etc.\n",
    "                [forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]\n",
    "    n_layers: Number of LSTM layers in the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers,bidirectional=True, lstm_drop=0.5,dropout=0.3):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(SentimentBidirectLSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout=lstm_drop,\n",
    "                            batch_first=False)\n",
    "    \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.Linear(hidden_dim*2, output_size)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, src_seq, seq_lengths):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        embeds = self.embedding(src_seq) # (max_seq_length, batch_size,embedding_dim)\n",
    "\n",
    "        packed = pack_padded_sequence(embeds,seq_lengths) \n",
    "        out,(hidden, cell)  = self.lstm(packed) # hidden: (n_layers * num directions,batch_size, hidden_dim)\n",
    "\n",
    "        # concat the last hidden layers of 2 directions\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.ln(hidden)\n",
    "        hidden = self.sig(hidden)\n",
    "        return hidden\n",
    "    \n",
    "#     def init_hidden(self, batch_size):\n",
    "#         hidden=torch.zeros(self.n_layers*2, batch_size, self.hidden_dim)\n",
    "#         return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T08:40:22.103426Z",
     "start_time": "2020-07-18T08:40:11.141253Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "9p5oay1s8pcP"
   },
   "outputs": [],
   "source": [
    "vocab_size, list_of_samples = preprocessed_data(data,labels)\n",
    "train_loader, test_loader, val_loader  = prepare_data_loader(list_of_samples,train_size=0.8,test_size=0.5)\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentBidirectLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers,bidirectional=True, lstm_drop=0.5,dropout=0.3)\n",
    "net = net.to(device)\n",
    "lr=0.001\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define train, evaluation, and test functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T08:40:56.169406Z",
     "start_time": "2020-07-18T08:40:56.163405Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "l0AWr0re8pcV"
   },
   "outputs": [],
   "source": [
    "def train_model(model,optimizer,criterion,train_loader):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        src_seq,src_seq_lengths,tgt_seqs= data\n",
    "        src_seq = torch.LongTensor(src_seq).to(device)\n",
    "        src_seq_lengths = torch.LongTensor(src_seq_lengths).to(device)\n",
    "        tgt_seqs=tgt_seqs.unsqueeze(dim=1)\n",
    "        tgt_seqs = torch.LongTensor(tgt_seqs).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(src_seq.T, src_seq_lengths).to(device)\n",
    "        loss = criterion(preds,tgt_seqs.float()).to(device)\n",
    "        acc = torch.sum(torch.round(preds) == tgt_seqs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss/len(train_loader), epoch_acc/(len(list_of_samples)*0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T08:40:56.439433Z",
     "start_time": "2020-07-18T08:40:56.432437Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "F1lYMgmc8pcY"
   },
   "outputs": [],
   "source": [
    "def evaluation_model(model,optimizer,criterion,val_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            src_seq,src_seq_lengths,tgt_seqs= data\n",
    "            src_seq = torch.LongTensor(src_seq).to(device)\n",
    "            src_seq_lengths = torch.LongTensor(src_seq_lengths).to(device)\n",
    "            tgt_seqs=tgt_seqs.unsqueeze(dim=1)\n",
    "            tgt_seqs = torch.LongTensor(tgt_seqs).to(device)\n",
    "\n",
    "            preds = model.forward(src_seq.T, src_seq_lengths).to(device)\n",
    "            loss = criterion(preds,tgt_seqs.float()).to(device)\n",
    "            acc = torch.sum(torch.round(preds) == tgt_seqs)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss/len(val_loader), epoch_acc/(len(list_of_samples)*0.2*0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WstkkRzowg1P"
   },
   "outputs": [],
   "source": [
    "def test_model(model,optimizer,criterion,test_loader):\n",
    "    the_model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            src_seq,src_seq_lengths,tgt_seqs= data\n",
    "            src_seq = torch.LongTensor(src_seq).to(device)\n",
    "            src_seq_lengths = torch.LongTensor(src_seq_lengths).to(device)\n",
    "            tgt_seqs=tgt_seqs.unsqueeze(dim=1)\n",
    "            tgt_seqs = torch.LongTensor(tgt_seqs).to(device)\n",
    "\n",
    "            preds = model.forward(src_seq.T, src_seq_lengths).to(device)\n",
    "            loss = criterion(preds,tgt_seqs.float()).to(device)\n",
    "            acc = torch.sum(torch.round(preds) == tgt_seqs)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return  epoch_loss/len(test_loader), epoch_acc/(len(list_of_samples)*0.2*0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T10:05:57.664423Z",
     "start_time": "2020-07-18T08:40:57.659909Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "WbA7XMaG8pcc",
    "outputId": "3133f39b-47e0-474c-dc98-4fc8b023b936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.486 Train Acc: 75.81% Val Loss: 0.324 Val Acc: 86.18%\n",
      "Epoch: 2 Train Loss: 0.262 Train Acc: 89.50% Val Loss: 0.271 Val Acc: 88.74%\n",
      "Epoch: 3 Train Loss: 0.147 Train Acc: 94.68% Val Loss: 0.292 Val Acc: 88.82%\n",
      "Epoch: 4 Train Loss: 0.078 Train Acc: 97.32% Val Loss: 0.331 Val Acc: 88.34%\n",
      "Epoch: 5 Train Loss: 0.033 Train Acc: 98.94% Val Loss: 0.500 Val Acc: 87.14%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss, train_acc = train_model(net,optimizer,criterion,train_loader)\n",
    "    val_loss, val_acc = evaluation_model(net,optimizer,criterion,val_loader)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} Train Loss: {train_loss:.3f} Train Acc: {train_acc*100:.2f}% Val Loss: {val_loss:.3f} Val Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save trained parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntrFXdTtGzs0"
   },
   "outputs": [],
   "source": [
    "PATH = \"trained_LSTM.pt\"\n",
    "torch.save(net.state_dict(),PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload trained parameters and to test the model performance on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HWZx_7Bk8pcg",
    "outputId": "05b3cebc-49f5-4db7-b2ce-024142ee48af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model =  SentimentBidirectLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers,bidirectional=True, lstm_drop=0.5,dropout=0.3)\n",
    "the_model.load_state_dict(torch.load(\"trained_LSTM.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1reHemo9wi2g",
    "outputId": "6e448922-03cb-4800-ae85-cc49ce604b84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.521 Test Acc: 87.02%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = test_model(net,optimizer,criterion,test_loader)\n",
    "print(f'Test Loss: {test_loss:.3f} Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
